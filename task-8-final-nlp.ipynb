{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from datasets import load_dataset\ndataset = load_dataset(\"conll2003\") \nlabel_list = dataset[\"train\"].features[\"ner_tags\"].feature.names","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-04T15:51:31.202819Z","iopub.execute_input":"2025-05-04T15:51:31.202976Z","iopub.status.idle":"2025-05-04T15:52:51.944951Z","shell.execute_reply.started":"2025-05-04T15:51:31.202962Z","shell.execute_reply":"2025-05-04T15:52:51.944375Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/12.3k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fcf7a6de0bdd4fbcbf50310e5df8eb78"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"conll2003.py:   0%|          | 0.00/9.57k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e2ea15dc97a2473783b05811392b227c"}},"metadata":{}},{"output_type":"stream","name":"stdin","text":"The repository for conll2003 contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/conll2003.\nYou can avoid this prompt in future by passing the argument `trust_remote_code=True`.\n\nDo you wish to run the custom code? [y/N]  y\n"},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/983k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b7e6a9f95f3b4165bff2ee6d21192cf4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/14041 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4a326272244c42a3a21927a90167ceab"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/3250 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6d74ae13e9784960aecaf454ed5c93e7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/3453 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"73d5fd6ed0fa4356a300bf3f9e3f556e"}},"metadata":{}}],"execution_count":1},{"cell_type":"code","source":"from transformers import RobertaTokenizerFast\n\ntokenizer = RobertaTokenizerFast.from_pretrained(\"roberta-base\", add_prefix_space=True)\n\ndef tokenize_and_align_labels(examples):\n    tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True)\n    labels = []\n    for i, label in enumerate(examples[\"ner_tags\"]):\n        word_ids = tokenized_inputs.word_ids(batch_index=i)\n        label_ids = []\n        previous_word_idx = None\n        for word_idx in word_ids:\n            if word_idx is None:\n                label_ids.append(-100)\n            elif word_idx != previous_word_idx:\n                label_ids.append(label[word_idx])\n            else:\n                label_ids.append(label[word_idx] if True else -100)\n            previous_word_idx = word_idx\n        labels.append(label_ids)\n    tokenized_inputs[\"labels\"] = labels\n    return tokenized_inputs\n\ntokenized_datasets = dataset.map(tokenize_and_align_labels, batched=True)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T15:53:18.431532Z","iopub.execute_input":"2025-05-04T15:53:18.431806Z","iopub.status.idle":"2025-05-04T15:53:29.079210Z","shell.execute_reply.started":"2025-05-04T15:53:18.431784Z","shell.execute_reply":"2025-05-04T15:53:29.078347Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aa3ca38c9fe94e69bfc3ea55e4389d2c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7724ec89f7f0482895dedcc5cfcffa90"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"22db1e8954874ce3925191e8ce9e8716"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"df712ed69e4343b9a92f94794cc41a43"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"554888c164e749cfb6156627fcc38f1d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/14041 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"19d986fb443e418f828e69e5d16ea719"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/3250 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f4d38afcce85441f87291b881f745edf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/3453 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b5a5a207123c48148795f21dcc492228"}},"metadata":{}}],"execution_count":2},{"cell_type":"code","source":"from transformers import RobertaForTokenClassification, TrainingArguments, Trainer\n\nmodel = RobertaForTokenClassification.from_pretrained(\"roberta-base\", num_labels=len(label_list))\n\nargs = TrainingArguments(\n    output_dir=\"./roberta-ner\",\n    eval_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    learning_rate=2e-5,\n    per_device_train_batch_size=8,\n    per_device_eval_batch_size=8,\n    num_train_epochs=3,\n    weight_decay=0.01,\n    logging_dir='./logs',\n    load_best_model_at_end=True,\n    metric_for_best_model=\"f1\",\n    greater_is_better=True,\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T15:53:29.080664Z","iopub.execute_input":"2025-05-04T15:53:29.080886Z","iopub.status.idle":"2025-05-04T15:53:49.914728Z","shell.execute_reply.started":"2025-05-04T15:53:29.080869Z","shell.execute_reply":"2025-05-04T15:53:49.913958Z"}},"outputs":[{"name":"stderr","text":"2025-05-04 15:53:34.601087: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1746374014.815719      31 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1746374014.876971      31 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nXet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/499M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bbffd5c70343433cbbe9a574d9c335ad"}},"metadata":{}},{"name":"stderr","text":"Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"!pip install seqeval\n!pip install evaluate\n","metadata":{"execution":{"iopub.status.busy":"2025-05-04T15:56:17.561179Z","iopub.execute_input":"2025-05-04T15:56:17.561678Z","iopub.status.idle":"2025-05-04T15:56:21.362674Z","shell.execute_reply.started":"2025-05-04T15:56:17.561652Z","shell.execute_reply":"2025-05-04T15:56:21.361738Z"}}},{"cell_type":"code","source":"import numpy as np\nimport evaluate\nmetric = evaluate.load(\"seqeval\")\n\ndef compute_metrics(p):\n    predictions, labels = p\n    predictions = np.argmax(predictions, axis=2)\n    true_labels = [[label_list[l] for l in label if l != -100] for label in labels]\n    true_predictions = [\n        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n        for prediction, label in zip(predictions, labels)\n    ]\n    return metric.compute(predictions=true_predictions, references=true_labels)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T15:56:23.048583Z","iopub.execute_input":"2025-05-04T15:56:23.049246Z","iopub.status.idle":"2025-05-04T15:56:23.730764Z","shell.execute_reply.started":"2025-05-04T15:56:23.049219Z","shell.execute_reply":"2025-05-04T15:56:23.730212Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/6.34k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"60d9cf6b795b4a77a22631caa930563d"}},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"trainer = Trainer(\n    model=model,\n    args=args,\n    train_dataset=tokenized_datasets[\"train\"],\n    eval_dataset=tokenized_datasets[\"validation\"],\n    tokenizer=tokenizer,\n    compute_metrics=compute_metrics,\n)\ntrainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T15:56:47.634216Z","iopub.execute_input":"2025-05-04T15:56:47.634527Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_31/1982903255.py:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Javascript object>","application/javascript":"\n        window._wandbApiKey = new Promise((resolve, reject) => {\n            function loadScript(url) {\n            return new Promise(function(resolve, reject) {\n                let newScript = document.createElement(\"script\");\n                newScript.onerror = reject;\n                newScript.onload = resolve;\n                document.body.appendChild(newScript);\n                newScript.src = url;\n            });\n            }\n            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n            const iframe = document.createElement('iframe')\n            iframe.style.cssText = \"width:0;height:0;border:none\"\n            document.body.appendChild(iframe)\n            const handshake = new Postmate({\n                container: iframe,\n                url: 'https://wandb.ai/authorize'\n            });\n            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n            handshake.then(function(child) {\n                child.on('authorize', data => {\n                    clearTimeout(timeout)\n                    resolve(data)\n                });\n            });\n            })\n        });\n    "},"metadata":{}}],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nlog_history = trainer.state.log_history\ntrain_loss = [log['loss'] for log in log_history if 'loss' in log]\neval_loss = [log['eval_loss'] for log in log_history if 'eval_loss' in log]\n\nplt.plot(train_loss, label='Train Loss')\nplt.plot(eval_loss, label='Eval Loss')\nplt.title(\"Training vs Evaluation Loss\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Loss\")\nplt.legend()\nplt.grid(True)\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}